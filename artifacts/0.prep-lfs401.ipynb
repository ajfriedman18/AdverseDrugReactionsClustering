{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We will be using the Drug Review Dataset (Drugs.com) for this workshop.\n",
    "\n",
    "Felix Gräßer, Surya Kallumadi, Hagen Malberg, and Sebastian Zaunseder. 2018. Aspect-Based Sentiment Analysis of Drug Reviews Applying Cross-Domain and Cross-Data Learning. In Proceedings of the 2018 International Conference on Digital Health (DH '18). ACM, New York, NY, USA, 121-125. DOI: [https://dl.acm.org/citation.cfm?doid=3194658.3194677]\n",
    "\n",
    "It is a part of the UCI machine learning repository.\n",
    "\n",
    "Dua, D. and Graff, C. (2019). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session=sagemaker.Session()\n",
    "sagemaker_bucket = sagemaker_session.default_bucket()\n",
    "bucket_name = 'lfs401-2019reinvent-public-cmh'\n",
    "source_key = 'drugsCom_raw.tsv'\n",
    "role = get_execution_role()\n",
    "\n",
    "source_prefix = 'source' \n",
    "reviews_data_prefix = 'reviews'\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's explore the source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_size(size):\n",
    "    splitter = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n",
    "    splitter_index = 0\n",
    "    while size > 1024 and splitter_index < 5:\n",
    "        splitter_index += 1\n",
    "        size = size/1024\n",
    "    size = '{:0.2f}'.format(size)\n",
    "    return '{}{}'.format(size,splitter[splitter_index])\n",
    "\n",
    "obj = s3.Object(bucket_name=bucket_name, key=source_prefix + '/' + source_key)\n",
    "obj_summary = s3.ObjectSummary(bucket_name=bucket_name, key=source_prefix + '/' + source_key)\n",
    "\n",
    "print(obj.key.split('/')[-1] +', size = '+ file_size(obj_summary.size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_file(file_name, prefix, index_col=None, usecols=None, parse_dates=None):\n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=prefix+'/'+file_name)\n",
    "    response_body = response[\"Body\"].read()\n",
    "    return pd.read_csv(io.BytesIO(response_body), header=0, delimiter=\"\\t\", low_memory=False,\n",
    "                      index_col=index_col, usecols=usecols, error_bad_lines=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note = source_key\n",
    "print (note)\n",
    "notes_partial = load_data_file(note, source_prefix)\n",
    "notes_partial.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_partial.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this demo, we will subselect 50 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_50 = notes_partial.sample(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_50.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, pick the number of topics to use\n",
    "\n",
    "Now that we have loaded the file, we will pick the number of topics to use. This has three steps:\n",
    "1. Use Amazon Comprehend Medical to identify topics in each review.\n",
    "2. Plot the distribution of the number of topics\n",
    "3. Randomly select the topics from each column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Identify topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_cnt = 1\n",
    "cm = boto3.client('comprehendmedical')\n",
    "\n",
    "topics_per_row = list()\n",
    "\n",
    "for index,row in notes_50.iterrows():\n",
    "    topic_list = []\n",
    "    \n",
    "    # For each row, use Comprehend Medical to detect entitites\n",
    "    entities = cm.detect_entities(Text=row['review'])\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Filter by Medical Condition\n",
    "    for entity in entities['Entities']:\n",
    "        if entity['Category'] == 'MEDICAL_CONDITION':\n",
    "            topic_list.append(entity['Text'])\n",
    "            topic_cnt += 1\n",
    "    topic_dict=dict(\n",
    "        Index=index,\n",
    "        DrugName=row['drugName'],\n",
    "        TopicList=topic_list\n",
    "    )\n",
    "    topics_per_row.append(topic_dict)\n",
    "    print (index)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_per_row[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Plot topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count = [ len(_['TopicList']) for _ in topics_per_row]\n",
    "plt.hist(topic_count,bins=20,label='Topic Count')\n",
    "plt.xlabel('Number of topics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Create topic list and review the output. In this example, we only use reviews with at least 5 germane topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=pd.DataFrame(columns=['drugName','topic1','topic2','topic3','topic4','topic5'])\n",
    "min_topic_len = 5\n",
    "\n",
    "for row in topics_per_row:\n",
    "    topic_list = row['TopicList']\n",
    "    if len(topic_list) >= min_topic_len:\n",
    "        # Randomly select topics\n",
    "        random_topics = random.choices(topic_list, k=min_topic_len)\n",
    "        reviews=reviews.append({\n",
    "            'drugName' : row['DrugName'],\n",
    "            'topic1' : str(random_topics[0]),\n",
    "            'topic2' : str(random_topics[1]),\n",
    "            'topic3' : str(random_topics[2]),\n",
    "            'topic4' : str(random_topics[3]),\n",
    "            'topic5' : str(random_topics[4])},\n",
    "            ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now get the number of reviews that met the criteria*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that the topics have been generated, save a CSV on both local disk as well as Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv('reviews_50.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3_client.upload_file('reviews_50.csv', sagemaker_bucket, reviews_data_prefix+'/reviews_50.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch preparation of drug name/topics\n",
    "\n",
    "Running a large number of calls serially through Amazon Comprehend Medical is, of course, inefficent. Comprehend Medical gives you the ability to batch process millions of notes in a single API call. For a high level overview, see https://aws.amazon.com/blogs/aws/introducing-batch-mode-processing-for-amazon-comprehend-medical/.\n",
    "\n",
    "## N.B. If you run the Comprehend Medical batch calculation across the entire dataset, you may incure charges, and the full analysis will complete after this workshop is complete. Do not do this as part of the workshop. We have provided files for you.\n",
    "\n",
    "Here are the steps to do this for this data set:\n",
    "1. For each line, extract the review and convert it to a JSON file named `<drug_id>.json` with the form `{\"Text\": \"...review...\"}`.\n",
    "2. Upload each of the files to S3 with a shared key space. For example: \n",
    "```\n",
    "lfs401-data/json/112.json\n",
    "lfs401-data/json/3528.json\n",
    "```\n",
    "3. Use Amazon Comprehend Medical Batch Mode to process the files. You can either use the console or the Comprehend Medical API. Save the output to a different key space. The job is submitted asynchronously so you can poll for the job status.\n",
    "4. Once the job is complete, you can access all the files in S3. Pull them back down to your instance to continue.\n",
    "\n",
    "\n",
    "## The post-processing is covered below. Resume with the following cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the output zip file from S3 and get the output path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp s3://lfs401-2019reinvent-public-cmh/output.tgz .\n",
    "aws s3 cp s3://lfs401-2019reinvent-public-cmh/source/drugsCom_raw.tsv source/\n",
    "tar xzf output.tgz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = os.getcwd()\n",
    "output_path = os.path.join(mypath,'output', [_ for _ in os.listdir('output') if os.path.isdir(os.path.join('output',_))][0])\n",
    "\n",
    "print ('Comprehend Medical Output Path: %s' % output_path)\n",
    "os.chdir(output_path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check number of files - should be 215063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -1 | grep -v Manifest | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, iterate through each file. If the file is a valid output name, add it to a dictionary. The file format should look similar to `12345.json.out`, where the numeric value is the id for the entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_from_file(filename):\n",
    "    with open(filename) as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = dict()\n",
    "counter = 0\n",
    "with os.scandir(output_path) as it:\n",
    "    for entry in it:\n",
    "        if not (entry.name.startswith('.') or entry.name.startswith('Manifest')) and entry.is_file():\n",
    "            d = get_topics_from_file(entry)\n",
    "            id = entry.name.split('.')[0]\n",
    "            \n",
    "            results_dict[id] = d\n",
    "        counter += 1\n",
    "        if counter % 10000 == 0:\n",
    "            print (counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in original files and iterate through to create a list of python dictionaries containing `id`, `drug name`, `condition`, and `review`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'drugsCom_raw.tsv'\n",
    "os.chdir(os.path.join(mypath, 'source'))\n",
    "orig_list = list()\n",
    "\n",
    "with open(filename) as csvfile:\n",
    "    myreader = csv.reader(csvfile, delimiter='\\t')\n",
    "    for row in myreader:\n",
    "        if row[0] == '':\n",
    "            continue\n",
    "        else:\n",
    "            orig_list.append({\n",
    "                'id': row[0],\n",
    "                'drugName': row[1],\n",
    "                'condition': row[2],\n",
    "                'review': row[3]\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm the length of the dictionary you lozaded in is the same as in the original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orig_list) == len(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, create a list for each entry that contains the index, drug name, and list of topics identified by Comprehend Medical. This is effectively an application inner join on index for `orig_list` and `result_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_per_row = list()\n",
    "topic_count = 1\n",
    "\n",
    "for entry in orig_list:\n",
    "    index = entry['id']\n",
    "    drugName = entry['drugName']\n",
    "    v = results_dict[index]\n",
    "\n",
    "    topic_list = []\n",
    "    for entity in v['Entities']:\n",
    "        if entity['Category'] == 'MEDICAL_CONDITION':\n",
    "            topic_list.append(entity['Text'])\n",
    "            topic_count += 1\n",
    "    \n",
    "    topic_dict = dict(\n",
    "        Index=index,\n",
    "        DrugName=drugName,\n",
    "        TopicList=topic_list\n",
    "    )\n",
    "    topics_per_row.append(topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topics_per_row) == len(orig_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_per_row[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now plot a histogram showing the distribution of number of topics per entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count = [ len(_['TopicList']) for _ in topics_per_row]\n",
    "plt.hist(topic_count,bins=20,label='Topic Count', range=(0,30))\n",
    "plt.xlabel('Number of topics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this example, we select 5 topics per entry. All entries with fewer than 5 relevant topics identified by Comprehend Medical are discarded. This yields ~80k distinct entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "min_topic_len = 5\n",
    "counter = 0\n",
    "review_list = []\n",
    "\n",
    "for row in topics_per_row:\n",
    "    topic_list = row['TopicList']\n",
    "    if len(topic_list) >= min_topic_len:\n",
    "        # Randomly select topics\n",
    "        random_topics = random.choices(topic_list, k=min_topic_len)\n",
    "        review_list.append({\n",
    "            'drugName' : row['DrugName'],\n",
    "            'topic1' : str(random_topics[0]),\n",
    "            'topic2' : str(random_topics[1]),\n",
    "            'topic3' : str(random_topics[2]),\n",
    "            'topic4' : str(random_topics[3]),\n",
    "            'topic5' : str(random_topics[4])})\n",
    "        \n",
    "reviews = pd.DataFrame(review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have identified the topics, save the full file, as well as subsamples to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_files_to_disk_and_s3(bucket_name, s3_uri_base, sample_nums, reviews):\n",
    "    for sample_num in sample_nums:\n",
    "        print (sample_num)\n",
    "        path = 'reviews_%ssample.csv' % str(sample_num)\n",
    "        write_file_to_disk_and_s3(bucket_name, s3_uri_base, path, sample_num, reviews)\n",
    "    print ('all reviews')\n",
    "    write_file_to_disk_and_s3(bucket_name, s3_uri_base, 'reviews_all.csv', reviews.shape[0], reviews)\n",
    "\n",
    "def write_file_to_disk_and_s3(bucket_name, s3_uri_base, path, sample_num, reviews):\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_key = s3_uri_base.rstrip('/') + '/' + path\n",
    "    sampled_reviews = reviews.sample(n=sample_num)\n",
    "    sampled_reviews.to_csv(path, header=True, index=False)\n",
    "    s3_client.upload_file(path, bucket_name, s3_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_numbers = [1000, 2000, 5000, 10000, 20000, 50000]\n",
    "s3_uri_base = 'reviews/'\n",
    "write_files_to_disk_and_s3(sagemaker_bucket, reviews_data_prefix, sample_numbers, reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congrats! You are done with data prep. Move to the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
