{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to clustering\n",
    "\n",
    "Clustering is an unsupervised machine learning technique that allows you to get insights about your data without the need of labels. It allows you find structure in a set of data points and allows you to group similar datapoints together. The output is represented as a plot that shows the clusters that each data point belogs to. Here is an example:\n",
    "\n",
    "\n",
    "source: [Wikipedia](https://en.wikipedia.org/wiki/Cluster_analysis)\n",
    "\n",
    "![](images/download1.png)\n",
    "\n",
    "In the above image, there are three distinct clusters that the data points are divided into. There is no set way to define the cluster requirements. It relies on the method chosen by the clustering algorithms to find the data points that are most similar and group them into a pre defined number of groups. The closer the datapoint to the center of the cluster, the stronger are its similarities to the others in the same cluster. The further it moves away from the center, the lesser are its similarities. There are ways to choose the appropriate number of clusters, which we will discuss in the next and final section of the workshop. For now, we will assume a predifined number of clusters for our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering\n",
    "k-means clustering algorithm attempts to partition a given set of data points to minimize the variance of the points in the partition.\n",
    "\n",
    "Given a set of data points $(x_1,x_2,x_3,....,x_n)$ , the algorithm aims to partition(cluster) these data points into $k$ clusters $(k<n)$ such that the sum of squared avarages (variance) of the points in the cluster is minimized.\n",
    "\n",
    "If we represent the clusters as $(c_1,c_2,c_3,....,c_k)$, and $\\gamma$ is the mean of all points in a cluster, then the alogorithm attempts to find\n",
    "\n",
    "$\\min\\sum_{i=1}^k\\sum_{x\\in c_i}\\parallel x-\\gamma_i\\parallel^2$\n",
    "\n",
    "where $\\parallel x-\\gamma_i\\parallel^2$ is the variance of point $x$ in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem definition\n",
    "As we saw in the previous notebook, the dataset we are working with has 5 adverse event topics for each drug name that were generated from unstructured descriptions of those adverse events. Given these unstructured descriptions as the only source of information about the drug, we want to cluster these drugs into groups based on how similar they are in terms of the adverse events that are associated with them. This means if two drugs result in similar adverse events, they will be grouped together. We will do this by using the K-means clustering algorithm described above.\n",
    "<br><br>Adverse events are reported in free text and this type of unsupervised grouping of drugs based on unstructured descriptions of adverse events  will allow analysts to better understand the adverse events patterns in groups of drugs just from raw unstructured data. Its valuable input for further studies involving supervised techniques like classification or regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We will begin by importing the necessary libraries into our notebook. **Note that [gensim](https://radimrehurek.com/gensim/) and [columnize](https://pypi.org/project/columnize/) doesnt come preinstalled with the SageMaker Notebook.** The library is preinstalled for you using a [SageMaker lifecycle configuration](https://docs.aws.amazon.com/sagemaker/latest/dg/notebook-lifecycle-config.html) that was attached to the notebook instance at the time it was created using a [CloudFormation template](https://aws.amazon.com/cloudformation/resources/templates/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker import get_execution_role\n",
    "import boto3, os\n",
    "import pandas as pd\n",
    "import gensim                                     \n",
    "import columnize\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from tempfile import TemporaryFile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a SageMaker session to manage interactions with SageMaker commands and other AWS services. We use this session to create a sagemaker default bucket, an S3 bucket in your AWS account that allows you to store training data, model arctifacts and other supporting files. We also get the sagemaker execution role and our s3 prefix for storing the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session=sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "role = get_execution_role()\n",
    "prefix = 'sagemaker/ae-clustering'\n",
    "\n",
    "print('Training input/output will be stored in {}/{}'.format(bucket, prefix))\n",
    "print('\\nIAM Role: {}'.format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis and pre processing\n",
    "Lets now load up our data file into a data frame by using the [`read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) command of Pandas. While we generated multiple files in the previous notebook, let's select the file `reviews_all.csv`, which contains all of the reviews generated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"source/reviews_all.csv\")\n",
    "display(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we drop the column \"drugName\" from the source dataframe and store the other columns in a new dataframe \"raw_sentences\". This is because we want to save the original dataframe for analyzing our output from the model. We then create a list of terms in columns \"topic1\" through \"topic5\" and save it in a new variable called \"corpus\". This variable contains a list of topics for all the drugs in our source data. You can see how the corpus looks for the first drug by printing the first row of the corpus (denoted by index 0).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences=raw_data.drop(['drugName'], axis=1)\n",
    "corpus = raw_sentences.values.tolist()\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating word embeddings and vocabulary\n",
    "\n",
    "Word embedding is a technique that allows words present in a corpus to be represented as a vector of real numbers with a fixed number of dimensions. This technique is invaluable for [Natural Language Processing (NLP)](https://en.wikipedia.org/wiki/Natural_language_processing) tasks as it allows algorithms to learn complex assiciations and context in a corpus of words. This allows you to answer questions like \"how similar/different is one word to the other word?\"<br><br> We will use the [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) algorithm from the Gensim library to generate real valued vector representations for our corpus of words. The vectors have 100 dimensions and the algorithm automatically generates the unique vocabulary for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traing wordtovec model on diagnosis description tokens\n",
    "word_vec = Word2Vec(corpus,iter = 50)\n",
    "\n",
    "ae_words = list(word_vec.wv.vocab)\n",
    "print(columnize.columnize(ae_words, displaywidth=80, ljust=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you took a subsample of the total review, each run through the workshop may generate a slightly different number of words in the vocabulary. The above model for all reviews has 4829 words in the vocabulary. You can get your number by checking the length of our vocabulary variable \"ae_words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ae_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we visualize these embeddings?\n",
    "Embeddings are much more intuitive when you can visualize them in a two dimensional plot. It allows you to view your vocabulary and see how closely they are associated with other words in the vocabulary.<br><br> To visualize vectors in a two dimensonal space, we perform the following steps:\n",
    "1. Reduce the number of dimensions in our source data to two. Remember that the Word2Vec algorithm generates a vector with a 100 dimensions. We need to bring that down to two so it can be plotted on the X and Y axis.\n",
    "2. Represent these two dimensional data points so that it maintains original pair wise association within the vocabulary. This allows words similar to each other to appear closer in two diemnsional space.\n",
    "\n",
    "To address the above conditions, we use a techniqe called [t-distributed stochastic neighbour embedding or t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding). The algorithm attempts to take high dimensional data sets and represents them in two dimensions so that objects that are similar are represented by nearby points and objects that are dissimilar are represented by distant points with high probability.<br><br>\n",
    "For our implementation, we chose the [TSNE algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) from the sklearn.manifold library. The below code block creates two arrays, the first one is called \"labels\" and it stores each word occurring in our vocabulary. The second array \"tokens\" stores the high dimensional vector representations of each word in the vocabulary. This second array is passed to the t-SNE algorithm to get a two dimensional representation of the coordinates computed by the t-SNE algorithm. This is stored into two arrays x and y. Finally, the code generates a scatter plot for (x,y) pairs with labels stored in the \"labels\" array.<br><br>\n",
    "For the purposes of our visualization, we will take **100 words** from our vocabulary and visulize them in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "tokens = []\n",
    "i=1\n",
    "\n",
    "for word in word_vec.wv.vocab:\n",
    "        tokens.append(word_vec[word])\n",
    "        labels.append(word)\n",
    "        i += 1 \n",
    "        if i==100:\n",
    "            break\n",
    "        \n",
    "    \n",
    "tsne_model = TSNE(perplexity=10, n_components=2, init='pca', n_iter=500, random_state=10)\n",
    "new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for value in new_values:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])\n",
    "\n",
    "plt.figure(figsize=(20, 20)) \n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    plt.annotate(labels[i],\n",
    "                 xy=(x[i], y[i]),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom',\n",
    "                fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the T-SNE Plot\n",
    "As described above, the plot provides a visual representation of the adverse reactions of drugs in a 2 dimensional space. The closer the terms are to each other, the more related they are. This allows you to see how the vocabulary is distributed overall. Lets zoom in to the plot and example some terms.\n",
    "\n",
    "Screen Shot 2019-11-21 at 3.34.45 PM![](images/download2.png)\n",
    "\n",
    "In the above, all sleep realted problems are appearing close to each other as they are closely realted. Zoom in on other parts of the plot and look at the various terms and how they appear on the plot. \n",
    "<br><br>\n",
    "Now that we have generated our word embeddings using the gensim Word2Vec algorithm, we can make use of functions like \"similar_to\" to find all terms in our vocabulary that are similar to a certain term. Here is an example of all terms similar to \"UTI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec.most_similar('UTI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try replacing UTI with other terms in the vocabulary and see the results.\n",
    "<br><br>\n",
    "Recall that the embeddings are vector representations of a word in a vocabulary. The model above stores these vectors as numpy arrays. We create a list of arrays for each word in our vocabulary in the variable \"vecs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = []\n",
    "for word in word_vec.wv.vocab:\n",
    "        vecs.append(word_vec[word])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how a single word is represented in our list of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our list of vectors, let's save it in a CSV file. We will use this file for training our k-means clustering algorithm. Note that because it is an unsupervised learning technique, we do not split the data into train and test. Instead, we use the same data to train a clustering model and for inference. During inference, the model returns a cluster label for each row in our dataset which allows us to group them together. To avoid any confusion, we create two variables: train_input for training and transform_input for inference. Note that both of them point to the same file on S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"vectors.csv\", vecs, delimiter=\",\")\n",
    "train_input = sagemaker_session.upload_data(path='vectors.csv', key_prefix='scikit-kmeans/data')\n",
    "transform_input = sagemaker_session.upload_data(path='vectors.csv', key_prefix='scikit-kmeans/data')\n",
    "print(train_input)\n",
    "print(transform_input)\n",
    "print(train_input == transform_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Training\n",
    "We will use SageMaker script mode to run our training. Script mode allows you to bring a training script and run it on Sagemaker with little modification. SageMaker runs this training in a container on an external instance and returns a trained model that can then be used for inference. The choice of container depends on the library being used in training. The library we are using for our training is Scikitlearn. Hence, SageMaker will run our training on [SageMaker Scikitlearn container](https://github.com/aws/sagemaker-scikit-learn-container). SageMaker also has similar containers for MxNet, TensorFlow and Pytorch. For reference, please look at the following link: https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html\n",
    "<br><br>\n",
    "We have prepared a python training script for our use case called \"condition_cluster.py\" Lets see the script in more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat condition_cluster.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script above parses the arguments that are used for training the model, trains the model and then saves it in the path model_dir. This is a default environment variable SM_MODEL_DIR that Sagemaker providess to be used in our training script. Similarly, Sagemaker provides environement variables for loading the input training data (SM_CHANNEL_TRAIN) and storing the model data during training (SM_OUTPUT_DATA_DIR). For a full list of environment variables, please look at the following link: https://github.com/aws/sagemaker-containers#important-environment-variables\n",
    "<br><br>\n",
    "These variables are handy to interact with the remote training container thats executing your training script.\n",
    "<br><br>\n",
    "The script also contains a function called model_fn. This function is reponsible for loading the trained model from the disk and making it ready for inference.\n",
    "<br><br>\n",
    "Let us now create a [Sagemaker Scikitlearn estimator](https://sagemaker.readthedocs.io/en/stable/sagemaker.sklearn.html) object using our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = 'condition_cluster.py'\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    train_instance_type=\"ml.c4.xlarge\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={'n_clusters': 3, 'random_state':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimator sklearn excepts the following parameters:\n",
    "* entry_point: The path to our training script\n",
    "* train_instance_type: The instance on which the Sagemaker will execute our training.\n",
    "* role: The role that will be used to run our training. This allows Sagemaker to determine if the user has permissions to run Sagemaker training.\n",
    "* sagemaker_session: The Sagemaker session using which Sagemaker executes the commands.\n",
    "* hyperparameters: A dictionary of hyperparameters that is used by Sagemaker training.\n",
    "\n",
    "\n",
    "**Note that we have hard coded the numeber of cluster to 3**. In the next notebook, we will see a way to tune our model to get the optimum number of clusers for our model. \n",
    "<br><br>\n",
    "We are now ready to begin training. We call the fit method passing the training input data path on S3 to begin training. **This will take a few minutes**. Please wait till the trainig completes. As the training proceeds, Sagemaker collects logs of the training job on cloudwatch logs. You can view them in the notebook as they are being produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.fit({'train': train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training completes, you will see a \"Billable seconds\" output as part of the logs. This the time that Sagemaker bills you for training. You can also see the outputs from your training script and the status of your training job. If everything ran successfully, you should see \"Reporting training\" variable saying \"SUCCESS\".\n",
    "<br><br>\n",
    "Sagemaker saves the model on S3 as a tar.gz file. You can look it up using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_boto3 = boto3.client('sagemaker')\n",
    "artifact = sm_boto3.describe_training_job(\n",
    "    TrainingJobName=sklearn.latest_training_job.name)['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "print('Model artifact persisted at ' + artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Inference\n",
    "Now that the training is complete, lets use our model to generate predictions. We will use [Sagemaker batch transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) for this. Batch transform allows you to run inference asynchronously. Its great for use cases where you do not need a persistent endpoint for your model but only require predictions to be generated in batches.\n",
    "<br><br>\n",
    "To achieve this, we create a [Sagemaker transformer](https://sagemaker.readthedocs.io/en/stable/transformer.html) called kmeans_transformer. It accepts the count of instances (1) you want to run inference on, the type of instance (ml.m4.xlarge), the data format to accept (text/csv), maximum payload size in MB (20) and maximum number of concurrent transforms(2). Once we create the transformer, we execute the `transform` method passing the path to our dataset (transform_input) and the content type of our data (text/csv).\n",
    "<br><br>\n",
    "**This will take a few minutes to complete**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_transformer = sklearn.transformer(instance_count=2, instance_type='ml.m4.xlarge', accept = 'text/csv', max_payload = 20, max_concurrent_transforms=2)\n",
    "\n",
    "kmeans_transformer.transform(transform_input, content_type='text/csv', split_type = 'Line')\n",
    "kmeans_transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the batch transform job completes, you are ready to download the inference predictions file from S3 and analyize it. We copy that file into a folder called `cluster_transformer/output/` and look at the first few lines of the file. As expected, the **file contains cluster labels (0,1,2) for the 3 clusters for each row in our dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = kmeans_transformer.output_path\n",
    "!mkdir -p cluster_transformer/output\n",
    "!aws s3 cp --recursive $batch_output/ cluster_transformer/output/\n",
    "# Head to see what the batch output looks like\n",
    "!head cluster_transformer/output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis\n",
    "Let's combine our resulting cluster labels with the original vocabulary terms and look at how they are distributed.\n",
    "<br><br>\n",
    "We begin by creating a `results` dataframe by combining words and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv('cluster_transformer/output/vectors.csv.out',header=None)\n",
    "vocab_df = pd.DataFrame(ae_words)\n",
    "results = pd.concat([vocab_df,predictions],axis=1)\n",
    "results.columns = [\"term\", \"label\"]\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a count plot to look at how the vocabulary is distributed between the three clusters. As you can see, cluster 2 (denoted by label 1) is the dominant cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"label\", data=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to visualize clusters is by using a scatter plot. It plots each data point in a two dimensional graph that allows you to visualize them as different groups. To do this, you need to convert your data points into sets of 2 (x and y coordinates) so they can be plotted. We use the dimensionality reduction technique [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) for this. Then we use it as an input to a scatter plot with color coding for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(transform_input,header=None)\n",
    "labels=results['label']\n",
    "pca = PCA(n_components=2)\n",
    "new_data= pca.fit_transform(data)\n",
    "plt.scatter(new_data[:, 0], new_data[:, 1], c=labels, s=20, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the three clusters, we see there is a clear saperation between the three groups with few datapoints overlapping in each region.\n",
    "<br><br>\n",
    "Let us now look at the first 10 terms in each of our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Cluster 1')\n",
    "print(results[results[\"label\"]==0]['term'].head(10))\n",
    "print('\\nCluster 2')\n",
    "print(results[results[\"label\"]==1]['term'].head(10))\n",
    "print('\\nCluster 3')\n",
    "print(results[results[\"label\"]==2]['term'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the results of our plots and the terms in each cluster provides us with some important insights:\n",
    "1. We were able to divide terms derived from unstructured drug reviews into 3 groups. \n",
    "2. Group 2 was the dominant group in our analysis.\n",
    "3. Examining the terms in each group, we are able to see some patterns emmerge. Cluster 1 seems to contain adverse events related to mood and emotions, cluster 2 has terms realated to infections and cluster 3 has terms related to mental health."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding drugs similar to each other based on adverse events\n",
    "If you recall, the end goal for us was to find all the drugs that are similar to a given drug with respect to the adverse events they demonstrate, given only unstructured review data and the names of drugs as inputs. With all the analysis we have done so far, we will not be able to tie the adverse events groups back to the original drugs in our dataset.\n",
    "<br><br>\n",
    "For the sake of this analysis, we will only take the first 200 drugs in our raw data. We will loop through the list of topics for each drug and find the corresponding list of labels for them from the predictions generated by our model. The resulting data contains the drug name and a list of topics and labels associated with each drug. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_new = pd.DataFrame(columns=['drugName','topics','labels'])\n",
    "raw_data_new=raw_data_new[0:0]\n",
    "for index,row in raw_data.head(200).iterrows():\n",
    "    labels=[]\n",
    "    \n",
    "    drug = row['drugName']\n",
    "    topics = [row['topic1'],row['topic2'],row['topic3'],row['topic4'],row['topic5']]\n",
    "    \n",
    "    for index,row in results.iterrows():\n",
    "        if row['term'] in topics:\n",
    "            labels.append(row['label'])\n",
    "    raw_data_new=raw_data_new.append({'drugName': drug,'topics': topics, 'labels':list(set(labels))},ignore_index=True)\n",
    "\n",
    "raw_data_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a function `similar_to`. This function takes a drug name as input and returns a list of drugnames that have any one of the associated labels. For example, if a drug contains labels [0,1], the function will return the names of all drugs that have either [0],[1] or [0,1] as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_to(drug_name):\n",
    "    import itertools\n",
    "    drugnames=[]\n",
    "    label= raw_data_new[raw_data_new['drugName']==drug_name]['labels']\n",
    "    label_list = list(itertools.chain(*label))\n",
    "    for index,row in raw_data_new.iterrows():\n",
    "        if row['drugName']!=drug_name and any(elem in label_list  for elem in row['labels']):\n",
    "            drugnames.append(row['drugName'])\n",
    "            \n",
    "    return drugnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now call the above function with a cetain drug name. The output will give you a list of drugs that are similar to the drug in the input. Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_drug = 'Remeron'\n",
    "output=similar_to(check_drug)\n",
    "print (\"\\n\\n\\nWe found \",len(output),\"drugs similar to \",check_drug,\" from a total of \",len(raw_data_new),\" drugs included in this analysis!\")\n",
    "print(\"\\n-------------------------------------------------------------------------------------------------------\\n\")\n",
    "print(columnize.columnize(output, displaywidth=150, ljust=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We have now achieved our goal of grouping drugs that are similar to each other based on the adverse events they had. We only had the unstructured adverse events data and drug names for this analysis. This opens the door to many important downstream analysis. The data from this algorithm can be used for correlative study of drug adverse events. In general, if there are drugs that are prescribed for a certain disease have similar adverse effects to drugs for a totally different disease, then they can be analyzed together. This could also be useful for supervised learning algorithms like classification which depend on labeled data.\n",
    "<br>\n",
    "We will now proceed to the third and final section of our workshop that will demonstrate a method to tune our clustering model. **Execute the next notebook `2.AE_Clustering_Automatic_Model_Tuning.ipynb`** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
